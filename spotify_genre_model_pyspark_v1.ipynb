{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-AdSIfoWn23",
        "outputId": "f35a2791-bae6-458f-dd83-0975bc4fdef7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
            "Requirement already satisfied: py4j==0.10.9.3 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "# Alternatively, if you want to install a specific version of pyspark:\n",
        "#!pip install pyspark==3.2.1 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "z22-NrJUW0OK"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator #, BinaryClassificationEvaluator \n",
        "\n",
        "# Basic libreries \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Pre-processing phase\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Features Importance\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# Model\n",
        "from sklearn import tree\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Hyper-Parameter Tuning\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-BncW7R_W2MD"
      },
      "outputs": [],
      "source": [
        "# Create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\").set('spark.executor.memory', '4G').set('spark.driver.memory', '45G').set('spark.driver.maxResultSize', '10G')\n",
        "\n",
        "# Create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "adXEIrmjW4UU",
        "outputId": "df46b387-8e05-462f-944d-4cf702d76268"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f06843eff50>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://8f364322f3d0:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSRJ15uuXB42",
        "outputId": "9c02cc52-8b6c-4506-aee2-b9375ea827c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "GDRIVE_DIR = \"/content/gdrive\" # Your own mount point on Google Drive\n",
        "GDRIVE_HOME_DIR = GDRIVE_DIR + \"/My Drive\" # Your own home directory\n",
        "GDRIVE_DATA_DIR = GDRIVE_HOME_DIR +  \"/Sapienza/Primo Anno/Big Data Computing/Project\"\n",
        "\n",
        "# Point Colaboratory to our Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(GDRIVE_DIR, force_remount=True)\n",
        "\n",
        "DATASET_URL = \"https://raw.githubusercontent.com/AndreaBe99/big-data-project/main/data/dataframe.csv\"\n",
        "GDRIVE_DATASET_FILE = GDRIVE_DATA_DIR + \"/\" + DATASET_URL.split(\"/\")[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VbzChbZ2XETx"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "\"\"\"\n",
        "This function downloads a file from a specific URL directly to Google Drive.\n",
        "\"\"\"\n",
        "def get_data(dataset_url, dest, chunk_size=1024):\n",
        "  response = requests.get(dataset_url, stream=True)\n",
        "  if response.status_code == 200: # Test if everything went ok\n",
        "    with open(dest, \"wb\") as file:\n",
        "      for block in response.iter_content(chunk_size=chunk_size): \n",
        "        if block: \n",
        "          file.write(block)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "JNmzPTCZXGRu",
        "outputId": "dec9878b-607d-4312-dc0d-84f025edfd88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving dataset from URL: https://raw.githubusercontent.com/AndreaBe99/big-data-project/main/data/dataframe.csv ...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                 \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: getresponse() got an unexpected keyword argument 'buffering'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ce71e9d07eca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Retrieving dataset from URL: {} ...\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_URL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_URL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGDRIVE_DATASET_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dataset successfully retrieved and stored at: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGDRIVE_DATASET_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-44c5e5e04c23>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(dataset_url, dest, chunk_size)\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\"\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Test if everything went ok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    528\u001b[0m         }\n\u001b[1;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in Python 3;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1373\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1374\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print(\"Retrieving dataset from URL: {} ...\".format(DATASET_URL))\n",
        "get_data(DATASET_URL, GDRIVE_DATASET_FILE)\n",
        "print(\"Dataset successfully retrieved and stored at: {}\".format(GDRIVE_DATASET_FILE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBYYcNfWXIEY"
      },
      "outputs": [],
      "source": [
        "spotify_tracks = spark.read.load(GDRIVE_DATASET_FILE, \n",
        "                           format=\"csv\", \n",
        "                           sep=\";\", \n",
        "                           inferSchema=\"true\", \n",
        "                           header=\"true\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Check the shape of the loaded dataset, i.e., number of rows and columns**"
      ],
      "metadata": {
        "id": "IPTd8ep9x74H",
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "98c32312-2214-4b25-8e7c-71d568d328a6"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The shape of the dataset is {:d} rows by {:d} columns\".format(spotify_tracks.count(), len(spotify_tracks.columns)))"
      ],
      "metadata": {
        "id": "G93XLf84bSom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Print out the schema of the loaded dataset**"
      ],
      "metadata": {
        "id": "-WC4RPQgyEsB",
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7da40297-6fc9-4e7a-a05c-95b71bed622a"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spotify_tracks.printSchema()"
      ],
      "metadata": {
        "id": "g8r6kRqTbd8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spotify_tracks.show(5)"
      ],
      "metadata": {
        "id": "1W9OucFAbo-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohe5M-dBkoPi"
      },
      "outputs": [],
      "source": [
        "spotify_tracks_pd = spotify_tracks.toPandas()\n",
        "\n",
        "spotify_tracks_pd['audio_avg_pitches'] = spotify_tracks_pd['audio_avg_pitches'].str.strip('][').str.split(', ')\n",
        "spotify_tracks_pd['audio_avg_timbre'] = spotify_tracks_pd['audio_avg_timbre'].str.strip('][').str.split(', ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eizwJqTkoPr"
      },
      "outputs": [],
      "source": [
        "split_pitch = pd.DataFrame(spotify_tracks_pd['audio_avg_pitches'].tolist(), columns=[\"pitch\" + str(i) for i in range(12)])\n",
        "split_pitch = split_pitch.astype(float)\n",
        "\n",
        "split_timbre = pd.DataFrame(spotify_tracks_pd['audio_avg_timbre'].tolist(), columns=[\"timbre\" + str(i) for i in range(12)])\n",
        "split_timbre = split_timbre.astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spotify_tracks_pd = pd.concat([spotify_tracks_pd, split_pitch], axis=1)\n",
        "spotify_tracks_pd = spotify_tracks_pd.drop('audio_avg_pitches', axis=1)\n",
        "\n",
        "spotify_tracks_pd = pd.concat([spotify_tracks_pd, split_timbre], axis=1)\n",
        "spotify_tracks_pd = spotify_tracks_pd.drop('audio_avg_timbre', axis=1)\n",
        "\n",
        "spotify_tracks_pd"
      ],
      "metadata": {
        "id": "MNnrrwqffyDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spotify_tracks_pd['album_release_date'] = pd.to_datetime(spotify_tracks_pd['album_release_date'])\n",
        "spotify_tracks_pd['year']= spotify_tracks_pd['album_release_date'].dt.year\n",
        "spotify_tracks_pd['month']= spotify_tracks_pd['album_release_date'].dt.month\n",
        "spotify_tracks_pd['day']= spotify_tracks_pd['album_release_date'].dt.day\n",
        "\n",
        "spotify_tracks_pd = spotify_tracks_pd.drop('album_release_date', axis=1)\n",
        "\n",
        "spotify_tracks_pd"
      ],
      "metadata": {
        "id": "VuaoID4gn4c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spotify_tracks_pd[\"track_explicit\"] = spotify_tracks_pd[\"track_explicit\"].astype(int)\n",
        "spotify_tracks_pd"
      ],
      "metadata": {
        "id": "uvjj-UWeoCc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spotify_tracks_pd = spotify_tracks_pd.drop('track_uri', axis=1)\n",
        "spotify_tracks_pd = spotify_tracks_pd.drop('id', axis=1)"
      ],
      "metadata": {
        "id": "SEXjx_5UoIVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spotify_tracks_pd.info()"
      ],
      "metadata": {
        "id": "WMlk1wXmoPhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's define some constants which we will use throughout this notebook\n",
        "NUMERICAL_FEATURES = []\n",
        "CATEGORICAL_FEATURES = []\n",
        "TARGET_VARIABLE = \"track_genre\"\n",
        "\n",
        "for col in spotify_tracks_pd.columns:\n",
        "  if spotify_tracks_pd.dtypes[col] == \"object\":\n",
        "    CATEGORICAL_FEATURES.append(col)\n",
        "  else:\n",
        "    NUMERICAL_FEATURES.append(col)\n",
        "\n",
        "CATEGORICAL_FEATURES.remove(TARGET_VARIABLE)\n",
        "print(\"Categorical: \", CATEGORICAL_FEATURES, \"\\nNumerical: \", NUMERICAL_FEATURES)"
      ],
      "metadata": {
        "id": "doZpwDUeb1Ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spotify_tracks = spark.createDataFrame(spotify_tracks_pd)"
      ],
      "metadata": {
        "id": "aJuMugIPq83I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spotify_tracks.printSchema()"
      ],
      "metadata": {
        "id": "ed3yT2tGX_1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# value counts of Batsman_Name column\n",
        "spotify_tracks.groupBy('album_release_date_precision').count().show()\n",
        "\n",
        "spotify_tracks.groupBy('track_name').count().show()\n",
        "\n",
        "spotify_tracks.groupBy('album_name').count().show()\n",
        "\n",
        "spotify_tracks.groupBy('artist_name').count().show()"
      ],
      "metadata": {
        "id": "1sDFiuAUkxjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 42\n",
        "# Randomly split our original dataset `house_df` into 80÷20 for training and test, respectively\n",
        "train_set, test_set = spotify_tracks.randomSplit([0.8, 0.2], seed=RANDOM_SEED)"
      ],
      "metadata": {
        "id": "Ktyl0QRqkZTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function is responsible to implement the pipeline above for transforming categorical features into numerical ones\n",
        "def to_numerical(df, numerical_features, categorical_features, target_variable):\n",
        "  \n",
        "  # 1. Label Encode target feature \n",
        "  stage_1= StringIndexer(inputCol=target_variable, outputCol='label')\n",
        "\n",
        "  # 2. Label Encode Categorical features\n",
        "  stage_2 = [StringIndexer(inputCol=c, outputCol=\"{0}_index\".format(c), handleInvalid=\"keep\") for c in categorical_features]\n",
        "\n",
        "  # 3. OneHot Encode \n",
        "  stage_3 = OneHotEncoder(inputCols=['album_release_date_precision_index'], outputCols=['album_release_date_precision_oh'])\n",
        "\n",
        "  # 4. create a vector of all the features required to train the logistic regression model \n",
        "  encoded_columns = ['track_name_index', 'album_name_index', 'artist_name_index', 'album_release_date_precision_oh']\n",
        "  stage_4 = VectorAssembler(inputCols= encoded_columns + numerical_features, outputCol='features')\n",
        "\n",
        "  # 4.a Create the StandardScaler\n",
        "  scaler = StandardScaler(inputCol=stage_4.getOutputCol(), outputCol=\"std_\"+assembler.getOutputCol(), withStd=True, withMean=True)\n",
        "\n",
        "  # 5. Populate the stages of the pipeline\n",
        "  stages = [stage_1] + stage_2 +[stage_3] + [stage_4] + [scaler]\n",
        "\n",
        "  # 6. Setup the pipeline with the stages above\n",
        "  pipeline = Pipeline(stages=stages)\n",
        "\n",
        "  # 7. Transform the input dataframe accordingly\n",
        "  transformer = pipeline.fit(df)\n",
        "  df_transformed = transformer.transform(df)\n",
        "\n",
        "  return df_transformed"
      ],
      "metadata": {
        "id": "oVWSiW_mf9-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the training set and get back both the transformer and the new dataset\n",
        "train_set_encoded = to_numerical(train_set, NUMERICAL_FEATURES, CATEGORICAL_FEATURES, TARGET_VARIABLE)\n",
        "\n",
        "# Select `features` and `label` (i.e., formerly `deposit`) target variable only\n",
        "train = train_set_encoded.select([\"features\", \"label\"])\n",
        "\n",
        "train.show(5, truncate=False)"
      ],
      "metadata": {
        "id": "5VdfijP2lWG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function defines the general pipeline for logistic regression\n",
        "def logistic_regression_pipeline(train, \n",
        "                                 numerical_features, \n",
        "                                 categorical_features, \n",
        "                                 target_variable, \n",
        "                                 with_std=True,\n",
        "                                 with_mean=True,\n",
        "                                 k_fold=5):\n",
        "  \n",
        "  stage_5_lg = LogisticRegression(featuresCol='features',labelCol='label')\n",
        "\n",
        "  logistic_regression_pipeline = Pipeline(stages= [stage_5_lg])\n",
        "\n",
        "  #### LOGISTIC REGRESSION\n",
        "  param_grid = ParamGridBuilder()\\\n",
        "    .addGrid(stage_5_lg.regParam, [0.01, 0.1, 1.0]) \\\n",
        "    .addGrid(stage_5_lg.maxIter, [10, 20, 50]) \\\n",
        "    .build()\n",
        "  # other param: .addGrid(stage_4_lg.elasticNetParam, [0.0, 0.25, 0.5, 0.75, 1.0])\n",
        "  cross_val_lg = CrossValidator(estimator=logistic_regression_pipeline,\n",
        "                                estimatorParamMaps=param_grid,\n",
        "                                evaluator=MulticlassClassificationEvaluator().setMetricName(\"accuracy\"), # default = \"areaUnderROC\", alternatively \"areaUnderPR\"\n",
        "                                numFolds=5,\n",
        "                                collectSubModels=True\n",
        "                                )\n",
        "  cv_model_lg = cross_val_lg.fit(spotify_tracks)\n",
        "\n",
        "  return cv_model_lg"
      ],
      "metadata": {
        "id": "jSn6OoIUmE5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_model = logistic_regression_pipeline(train, NUMERICAL_FEATURES, CATEGORICAL_FEATURES, TARGET_VARIABLE)\n",
        "\n",
        "# Make predictions on the test set (`cv_model` contains the best model according to the result of k-fold cross validation)\n",
        "# `test_df` will follow exactly the same pipeline defined above, and already fit to `train_df`\n",
        "test_predictions = cv_model.transform(test_set)\n",
        "\n",
        "test_predictions.select(\"features\", \"prediction\", \"label\").show(5)"
      ],
      "metadata": {
        "id": "PHYqzIlOm4wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define stage 5: logistic regression model                          \n",
        "stage_5_lg = LogisticRegression(featuresCol='features',labelCol='label')\n",
        "stage_5_dr = DecisionTreeClassifier(featuresCol='features',labelCol='label')\n",
        "stage_5_rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "# setup the pipeline\n",
        "logistic_regression_pipeline = Pipeline(stages= [stage_1] + stage_2 +[stage_3] + [stage_4] + [stage_5_lg])\n",
        "decision_tree_pipeline = Pipeline(stages= [stage_1] + stage_2 +[stage_3] + [stage_4] + [stage_5_dr])\n",
        "random_forest_pipeline = Pipeline(stages= [stage_1] + stage_2 +[stage_3] + [stage_4] + [stage_5_rf])"
      ],
      "metadata": {
        "id": "f0gIujrVcpPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### LOGISTIC REGRESSION\n",
        "param_grid = ParamGridBuilder()\\\n",
        "  .addGrid(stage_5_lg.regParam, [0.01, 0.1, 1.0]) \\\n",
        "  .addGrid(stage_5_lg.maxIter, [10, 20, 50]) \\\n",
        "  .build()\n",
        "# other param: .addGrid(stage_4_lg.elasticNetParam, [0.0, 0.25, 0.5, 0.75, 1.0])\n",
        "cross_val_lg = CrossValidator(estimator=logistic_regression_pipeline,\n",
        "                              estimatorParamMaps=param_grid,\n",
        "                              evaluator=MulticlassClassificationEvaluator().setMetricName(\"accuracy\"), # default = \"areaUnderROC\", alternatively \"areaUnderPR\"\n",
        "                              numFolds=5,\n",
        "                              collectSubModels=True\n",
        "                              )\n",
        "cv_model_lg = cross_val_lg.fit(spotify_tracks)\n",
        "\n",
        "# transform the data\n",
        "final_spotify_tracks_lg = cv_model_lg.transform(spotify_tracks)"
      ],
      "metadata": {
        "id": "MDwh5wA3c38H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### DECISION TREE\n",
        "param_grid = ParamGridBuilder()\\\n",
        "  .addGrid(stage_5_dr.maxDepth, [3, 5, 8]) \\\n",
        "  .addGrid(stage_5_dr.impurity, [\"gini\", \"entropy\"]) \\\n",
        "  .build()\n",
        "cross_val_dt = CrossValidator(estimator=decision_tree_pipeline,\n",
        "                              estimatorParamMaps=param_grid,\n",
        "                              evaluator=MulticlassClassificationEvaluator().setMetricName(\"accuracy\"), # default = \"areaUnderROC\", alternatively \"areaUnderPR\"\n",
        "                              numFolds=5,\n",
        "                              collectSubModels=True\n",
        "                              )\n",
        "cv_model_dt = cross_val_dt.fit(spotify_tracks)\n",
        "\n",
        "# transform the data\n",
        "final_spotify_tracks_dt = cv_model_dt.transform(spotify_tracks)"
      ],
      "metadata": {
        "id": "8p5rfrNZdBmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### RANDOM FOREST\n",
        "param_grid = ParamGridBuilder()\\\n",
        "  .addGrid(stage_5_rf.maxDepth, [3, 5, 8]) \\\n",
        "  .addGrid(stage_5_rf.numTrees, [10, 50, 100]) \\\n",
        "  .build()\n",
        "cross_val_rf = CrossValidator(estimator=random_forest_pipeline, \n",
        "                            estimatorParamMaps=param_grid,\n",
        "                            evaluator= MulticlassClassificationEvaluator().setMetricName(\"accuracy\") # default = \"areaUnderROC\", alternatively \"areaUnderPR\"\n",
        "                            numFolds=5,\n",
        "                            collectSubModels=True \n",
        "                            )\n",
        "cv_model_rf = cross_val_rf.fit(spotify_tracks)\n",
        "\n",
        "# transform the data\n",
        "final_spotify_tracks_rf = cv_model_rf.transform(spotify_tracks)"
      ],
      "metadata": {
        "id": "nEEi6ORldDxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_spotify_tracks_lg.select(\"features\", \"prediction\", \"track_genre_index\").show(5)"
      ],
      "metadata": {
        "id": "3aixZkO2Zgr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_spotify_tracks_dt.select(\"features\", \"prediction\", \"track_genre_index\").show(5)"
      ],
      "metadata": {
        "id": "yePaguORZlCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_spotify_tracks_rf.select(\"features\", \"prediction\", \"track_genre_index\").show(5)"
      ],
      "metadata": {
        "id": "-5LGHqDpZmUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view some of the columns generated\n",
        "# final_spotify_tracks.select('features', 'track_genre_index', 'rawPrediction', 'probability', 'prediction').take(10)"
      ],
      "metadata": {
        "id": "oRxU-rGqv4r6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "spotify_genre_model_pyspark.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}